# iFood Case Study

Este projeto contÃ©m a anÃ¡lise e modelagem de dados para um case study do iFood.

## Estrutura do Projeto

```
ifood-case/
â”œâ”€â”€ data/                       # Datasets
â”‚   â”œâ”€â”€ raw/                   # Dados originais (JSON)
â”‚   â”‚   â”œâ”€â”€ offers.json        # Dados de ofertas
â”‚   â”‚   â”œâ”€â”€ profile.json       # Dados de perfil dos usuÃ¡rios
â”‚   â”‚   â””â”€â”€ transactions.json  # Dados de transaÃ§Ãµes
â”‚   â””â”€â”€ processed/             # Dados processados (Parquet)
â”‚       â””â”€â”€ offers_processed.parquet/  # Ofertas processadas
â”œâ”€â”€ notebooks/                 # Jupyter notebooks para anÃ¡lise
â”‚   â”œâ”€â”€ 1_data_processing.ipynb    # Processamento e limpeza dos dados
â”‚   â”œâ”€â”€ 2_data_exploratory.ipynb  # AnÃ¡lise exploratÃ³ria dos dados
â”‚   â”œâ”€â”€ 3_modeling.ipynb           # Modelagem e machine learning
â”‚   â””â”€â”€ artifacts/                 # Artefatos do Spark
â”œâ”€â”€ src/                       # CÃ³digo fonte modular
â”‚   â””â”€â”€ utils.py              # FunÃ§Ãµes utilitÃ¡rias (Spark, carregamento de dados)
â”œâ”€â”€ presentation/              # Slides e materiais para stakeholders
â”œâ”€â”€ README.md                 # Este arquivo
â””â”€â”€ requirements.txt          # DependÃªncias do projeto
```

## Como Usar

### 1. ConfiguraÃ§Ã£o do Ambiente

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd ifood-case

# Criar ambiente virtual (recomendado)
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou .venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Estrutura de ExecuÃ§Ã£o

Os notebooks devem ser executados na seguinte ordem:

1. **`1_data_processing.ipynb`** - Processamento inicial dos dados
2. **`2_data_exploratory.ipynb`** - AnÃ¡lise exploratÃ³ria 
3. **`3_modeling.ipynb`** - Modelagem e machine learning

### 3. Tecnologias Utilizadas

- **PySpark**: Processamento distribuÃ­do de dados em larga escala
- **Pandas**: ManipulaÃ§Ã£o e anÃ¡lise de dados
- **Scikit-learn**: Machine learning (modelos de ensemble)
- **LightGBM**: Gradient boosting otimizado
- **Matplotlib/Seaborn**: VisualizaÃ§Ã£o de dados

### 4. ExecuÃ§Ã£o

```bash
# Iniciar Jupyter
jupyter notebook

# Ou usar Jupyter Lab
jupyter lab
```

## Notebooks

### ğŸ“Š 1_data_processing.ipynb
- **Objetivo**: Processamento e limpeza dos dados brutos
- **Principais atividades**:
  - ConfiguraÃ§Ã£o do ambiente PySpark
  - Carregamento dos dados JSON (offers, profile, transactions)
  - Limpeza e transformaÃ§Ã£o dos dados
  - Salvamento dos dados processados em formato Parquet
- **SaÃ­das**: Dados processados na pasta `data/processed/`

### ğŸ” 2_data_exploratory.ipynb  
- **Objetivo**: AnÃ¡lise exploratÃ³ria detalhada dos dados
- **Principais atividades**:
  - AnÃ¡lise estatÃ­stica descritiva
  - VisualizaÃ§Ãµes de distribuiÃ§Ãµes e correlaÃ§Ãµes
  - IdentificaÃ§Ã£o de padrÃµes nos dados
  - AnÃ¡lise de ofertas, perfis de usuÃ¡rios e transaÃ§Ãµes
- **Ferramentas**: Matplotlib, Seaborn para visualizaÃ§Ãµes

### ğŸ¤– 3_modeling.ipynb
- **Objetivo**: Modelagem preditiva e causal inference
- **Principais atividades**:
  - PreparaÃ§Ã£o dos dados para modelagem
  - ImplementaÃ§Ã£o de modelos de machine learning
  - AnÃ¡lise de uplift/causal inference usando T-Learner
  - AvaliaÃ§Ã£o de modelos (AUC, mÃ©tricas de regressÃ£o)
  - InterpretaÃ§Ã£o dos resultados
- **Modelos**: Random Forest, Gradient Boosting, T-Learner para uplift modeling


## Dados

### Datasets DisponÃ­veis

#### ğŸ“ raw/ - Dados Originais
- **`offers.json`**: InformaÃ§Ãµes sobre ofertas/campanhas promocionais
- **`profile.json`**: Dados demogrÃ¡ficos e comportamentais dos usuÃ¡rios
- **`transactions.json`**: HistÃ³rico de transaÃ§Ãµes dos clientes

#### ğŸ“ processed/ - Dados Processados
- **`offers_processed.parquet/`**: Dados de ofertas limpos e estruturados em formato Parquet para otimizaÃ§Ã£o de performance

### CaracterÃ­sticas dos Dados
- **Formato original**: JSON (dados semi-estruturados)
- **Formato processado**: Parquet (colunar, otimizado para analytics)
- **Processamento**: PySpark para escalabilidade
- **Armazenamento**: Local filesystem com possibilidade de migraÃ§Ã£o para cloud storage

### ConsideraÃ§Ãµes TÃ©cnicas
- Os dados brutos sÃ£o mantidos intactos para rastreabilidade
- Formato Parquet oferece compressÃ£o e query performance otimizada
- Pipeline de processamento documented nos notebooks